\documentclass[letterpaper,final,12pt,reqno]{amsart}

\usepackage[total={6.3in,9.2in},top=1.1in,left=1.1in]{geometry}

\usepackage{times,bm,bbm,empheq,fancyvrb,graphicx,amsthm,amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{longtable}
\usepackage{booktabs}

\usepackage{tabto}
\TabPositions{1.5cm}

\usepackage{float}

% hyperref should be the last package we load
\usepackage[pdftex,
colorlinks=true,
plainpages=false, % only if colorlinks=true
linkcolor=blue,   % ...
citecolor=Red,    % ...
urlcolor=black    % ...
]{hyperref}

\renewcommand{\baselinestretch}{1.05}

\allowdisplaybreaks[1]  % allow display breaks in align environments, if they avoid major underfull

\newcommand{\eps}{\epsilon}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\grad}{\nabla}
\newcommand{\Div}{\nabla\cdot}
\newcommand{\trace}{\operatorname{tr}}

\newcommand{\hbn}{\hat{\mathbf{n}}}

\newcommand{\bb}{\mathbf{b}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bbf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

\newcommand{\bF}{\mathbf{F}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bX}{\mathbf{X}}

\newcommand{\bxi}{\bm{\xi}}
\newcommand{\bzero}{\bm{0}}

\newcommand{\cK}{\mathcal{K}}
\newcommand{\cV}{\mathcal{V}}

\newcommand{\rhoi}{\rho_{\text{i}}}

\newcommand{\ip}[2]{\left<#1,#2\right>}

\newcommand{\maxR}{R^{\bm{\oplus}}}
\newcommand{\minR}{R^{\bm{\ominus}}}
\newcommand{\iR}{R^{\bullet}}

\newcommand{\nn}{{\text{n}}}
\newcommand{\pp}{{\text{p}}}
\newcommand{\qq}{{\text{q}}}
\newcommand{\rr}{{\text{r}}}

\newcommand{\supp}{\operatorname{supp}}
\newcommand{\Span}{\operatorname{span}}


\newenvironment{review}%
{\bigskip \par \begin{quote} \selectfont \sl}%
{\end{quote}}

\newcommand\short[1]{\medskip\noindent #1}   % short form of response

\newenvironment{response}% long form of response
{\medskip\noindent}%
{}


\begin{document}
\title[Response to reviews]{Response to reviews of \\ \emph{A full approximation scheme multilevel method \\ for nonlinear variational inequalities}}

\author{Ed Bueler}

\author{Patrick Farrell}

\date{\today}

\maketitle

%\tableofcontents

\thispagestyle{empty}
%\bigskip

The thoughtful comments from the two reviewers are much appreciated.  We have revised the manuscript in response. For the convenience of the Editor and Referees we include a latexdiff to show the specific changes.


\section{Responses to Referee 1}

%%% ELB CONFIDENTIAL  I am guessing this is Tai himself.

\begin{review}
This paper is about using multilevel algorithm to solve nonlinear problems with constraints. Many real-world applications need to solve problem of this kind. Thus, the fast algorithm proposed here is of great importance.

This work proposed to improve the CD (constraint decomposition) method of [32] in several aspects. The new techniques are innovative (see detailed comments below) and show very good performances in numerical experiments report in this paper. I strongly suggest accepting this work for SISC after some minor revisions suggested below.
\end{review}

\short{Thank you for this supportive and accurate summary.}

\begin{review}
The reasons that I suggest accepting this paper are based on the following observations:

1. This paper proposed a new way to decompose the constraint set as shown in section 5. This is a good and innovative idea. In the original algorithm of [32], all the decompositions need to go to the finest level. With this new technique, only the nearest level of grid needs to be involved. This is a great advantage.

2. Another advantage of the method proposed in this paper is to use FAS (full approximation Scheme) to solve the subproblems over the different levels, while the original scheme of [32] needs to solve a nonlinear problems over the different grid levels that is projected from the finest mesh and also exactly (even though, approximation solver can be considered as in Tai-Espedal (SINUM 1998). This is making the coarse grid solving much fast and cheaper.

3. This paper proposed to use the algorithm for general monotone nonlinear problems with constraints, while the algorithm proposed in [32] is only for minimization problems. The operator from convex minimization problems is monotone, but there are many monotone operators that are not from minimization problems as shown in Example 2.5 and 2.6 of this work.
\end{review}

\begin{response}
Thank you for raising these points. We now emphasize that [32] requires visiting the finest level in the introduction.

We also appreciate the pointer to Tai \& Espedal (1998), which shows convergence of a space decomposition method when each subspace solver is only approximate (and satisfying certain bounds).  This could form an important part of an eventual proof of convergence.
\end{response}

\begin{review}
The weak point of this work is that there is no convergence proof for the proposed algorithms. Intuitively, the constraint decomposition given in this paper is much tighter. It would give a better correction during the iterations, but this is not shown theoretically nor numerically.
\end{review}

\begin{response}
We agree that this is the weakest point of the work.  Sadly, a convergence proof is not yet available.  We believe that the algorithm is sufficiently powerful as to be worthwhile publishing, and that our numerical demonstrations adequately support this claim. We also hope that by publishing this algorithm we might stimulate theoretical investigations by others.
\end{response}


\begin{review}
The proposed FMG solver is an optimal solver as claimed by the authors. It also improves CD method of [32] in several aspects. The authors need to supply comparisons between the two methods in the numerical section.
\end{review}

\short{Our numerical demonstrations support the claim of optimality, and the reviewer seems to agree.  We already supply comparisons in Section 8; in Example 8.1 we compare to results from [22] (Graeser \& Kornhuber, 2009), who have implemented the [32] (Tai, 2003) method using a constraint decomposition like ours, whereas the results in [32] use rather different constraint decompositions.  Our achieved convergence rate is about $0.10$, comparable to those in [22], whereas [32] achieves $>0.70$ on the same ``ball'' problem at comparable resolution.  (Smaller numbers are better here.)  On the other hand, the improvement seen in this simplest Laplacian obstacle problem is largely from our better smoother.}

\begin{review}
Good if the authors can plot the error of the numerical solutions, i.e. $u^J - u^*$ and its convergence order with respect to mesh size $h_J$ with different mesh size $h_J$. This observed convergence order could be beneficial for people working with numerical analysis.
\end{review}

\short{We now report the convergence order of $\|u^\star - u^J\|$ for the two cases where the exact solution is known, namely the ``ball'' problem in Example 8.1 and the $p$-Laplacian problem in Example 8.2.}


\section{Responses to Referee 2}

\begin{review}
This paper merges the FAS multigrid method, which is a highly optimal solver, with a particular optimization method (CD).  They are able to get close to optimal work complexity this class of optimization solvers, which is a significant accomplishment.  The paper is clearly written and demonstrates good performance.
\end{review}

\short{This summary is accurate and appreciated.}

\begin{review}
Abstract: ``FASCD is a common extension of \dots'' this sounds like FASCD is, well, common, yet this seems to be the first paper that develops this method.  I think you are saying others have done the same, but it reads like FASCD is common.  Maybe something like ``while many multilevel VI solvers have been developed, FASCD has unique desirable properties''.
\end{review}

\short{Ah, we meant ``common'' as in ``joint'' rather than ``frequent''. The word ``common'' has been replaced by ``joint'' for clarity.  All that is intended is that FASCD extends two previously-separate algorithmic ideas, namely FAS and CD.  Note that the desirable properties are listed clearly in the Introduction.}

\begin{review}
The prior work section starting at line 40, is hard to follow.  There are three projects that you comparing to.  There are many qualities discussed.  A table or some graphic might be useful and some signposts like ``three MG VI methods have been developed in prior works [6,24,22]'' and a paragraph for each.
\end{review}

\short{Thank you for this helpful suggestion. Indeed we seek to contrast with four projects, i.e.~[6,32,22,24] (old numbering), which makes the reviewer's point!  This part of the text is now an itemized list, thus easier for the reader to follow.}

\begin{review}
You might move all discussion of desirable properties to the beginning at line 25. and refer to numbered properties in the prior work section as far as what other do well and don't do well.
\end{review}

\short{While keeping the overall structure mostly the same, we have made an effort to clean and clarify the presentation of desirable properties (an enumerated list) and of prior work (an itemized list).}

\begin{review}
The FAS treatment, while good, is a little confusing.  Perhaps only to an expert.
\end{review}

\short{We have made an effort to clarify the presentation of the FAS idea. Regarding existing literature, it remains a relatively awkward idea to explain; apparently our presentation is not exceptional.}

\begin{review}
FMG is not an iterative method, yet you start with ``FASCD \dots by a small, mesh-independent number of FMG iterations \dots''.

The FAS algorithm with V-cycles attached to make an iterative method to arbitrary solver tolerance is fine, but reporting multiple iteration is not really correct.  Right?  It is FMG + k V-cycle iterates.  Maybe report ``extra V-cycles in FASCD'', which should be 0, but you ``over solve'' to compare with others.  This is subtle and maybe could be put in the caption of Table 2 to reduce the text for the column heading from what is now ``FMG'' (``FMG V-cycles'' maybe).
\end{review}

\short{The reviewer is correct, and we have settled on a presentation close to that suggested.  As explained now in the text, notation ``$+n$'' is now used when $n$ additional V-cycles were required to satisfy the stopping criterion (7.3).  (This is documented in Section 7 before Algorithm 7.1, and at first use in Example 8.1.)  Thus ``$+0$'' appears when the FMG cycle itself sufficed, e.g.~(new) Table 3. }


\begin{review}
This issue becomes a real problem in Table 5 where you devide the solver time by what you call the FASCD iterations.  This does not make sense. I would suggest, since you are just looking at time per ``iteration'', that you just do full MG and forget about iterating.
\end{review}

\short{The reviewer is correct that in Table 5 the time-divided-by-iterations column was not helpful, and we have removed it.  The ``iterations'' column is now ``$+n$'' as described above.  We believe this addresses the reviewer's very reasonable criticism.}

\begin{review}
FASCD is different in that you have to do strange things with the constraints in the down smoothing, so it is not surprising that V(1,0) fails (table 3) and V(1,1) is WORSE that V(0,1) for on test, which indicates a certain fragility with the down smoothing process. It seems that should be discussed, but I don't see this.
\end{review}

\begin{response}
The description of ``strange things with the constraints in the down smoothing'' is understandable, but this is inherited from reference [22].  As we try to explain in Section 5, in down-smoothing the constraint sets must be small; this characterizes the cycles in [22].  We have improved upon [22] by recognizing that such small sets can be avoided in up-smoothing.

One point of the Example in question (i.e.~Example 8.2) is, indeed, that small constraint sets are bad for the kind of highly nonlinear VI problems we are interested in solving.  (Note that [22] does not address $p$-Laplacian cases, for other than the classical $p=2$ case, while we address the $1.5$-Laplacian and a doubly-degenerate version of the $4$-Laplacian.)  Exactly why convergence is damaged by too-small sets, in a case like this, might be revealed by a convergence proof, i.e.~the proof we don't have.

The subtle and very grid-dependent difference between the better V(0,1) and V(1,1) cycles is also something we do not understand.  It may be connected to the location of the free boundary relative to the nearest grid point, but this is too speculative for us to feel comfortable including as a remark in the manuscript.
\end{response}

\end{document}
